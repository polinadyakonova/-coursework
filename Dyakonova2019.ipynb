{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "import pymorphy2\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = webdriver.Firefox(executable_path=r'D:\\geckodriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month(j):\n",
    "    if j < 10:\n",
    "        return '0' + str(j)\n",
    "    return str(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка по годам\n",
    "for i in range(1900, 2000):\n",
    "    source_code = ''\n",
    "    for j in range(1, 13):\n",
    "        wd.get('http://prozhito.org/notes?date=\"' + str(i) + '-' + month(j) + '-01\"&dateTop=\"' + str(i) + '-' + month(j) + '-28\"')\n",
    "        elem = wd.find_element_by_xpath(\"//*\")\n",
    "        time.sleep(4)\n",
    "        source_code = source_code + elem.get_attribute(\"outerHTML\")\n",
    "    with open('data_year/diaries' + str(i) + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        f.write(source_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#парсинг веб-страниц, удаляем теги и т.д.\n",
    "def diary_parsing(name):\n",
    "    with open('data_year/' + name + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "        diary = f.read()\n",
    "    if diary.find('span') == -1:\n",
    "        return\n",
    "    diary = diary.split('span class=\"note-date\"')\n",
    "    diary[0] = ''\n",
    "    for i in range (0, len(diary)):\n",
    "        if diary[i].find('...открыть') > -1:\n",
    "            diary[i] = ''\n",
    "        diary[i] = re.sub('[\\da-zA-Z<>=\"&$/()!,_:;\\?«»\\*\\—\\.\\[\\]]', '', diary[i])\n",
    "    diary = ' '.join(diary)\n",
    "    diary = re.sub('-закрыть -', '', diary)\n",
    "    diary = re.sub(' \\-+', '', diary)\n",
    "    a = diary.find('Хотите помочь?')\n",
    "    with open('Prepared/pr_' + name + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        f.write(diary[:a])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\?\n",
      "\n",
      "<>:7: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\?\n",
      "\n",
      "<>:7: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\?\n",
      "\n",
      "<ipython-input-23-0cacbf40744f>:7: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#приводим слова к начальной форме\n",
    "def diary_lemmatize(name):\n",
    "    with open('Prepared/pr_' + name + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "        diary = f.read()\n",
    "    diary = diary.split(' ')\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    for i in range(0, len(diary)):\n",
    "        if diary[i][1:].islower() and morph.parse(diary[i])[0].tag.POS == \"NOUN\":\n",
    "            diary[i] = morph.parse(diary[i].lower())[0].normal_form\n",
    "        else:\n",
    "            diary[i] = ''\n",
    "    with open('Prepared/pr_' + name + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        f.write(' '.join(diary))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#для просмотра самых частых слов\n",
    "def word_counter(name):\n",
    "    with open('Prepared/pr_' + name + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "        diary = f.read().split(' ')\n",
    "    yeardict = {}\n",
    "    with open('stop_words.txt', 'r', encoding = 'utf-8') as f:\n",
    "        stop = f.read().split('\\n')\n",
    "    for d in diary:\n",
    "        if len(d) > 2 and d not in stop:\n",
    "            try:\n",
    "                yeardict[d] += 1\n",
    "            except:\n",
    "                yeardict[d] = 1\n",
    "    with open('Dicts/dict_' + name + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        for w in sorted(yeardict, key=yeardict.get, reverse=True):\n",
    "            if yeardict[w] < 10:\n",
    "                break\n",
    "            f.write(w + ' ' + str(yeardict[w]) + '\\n')    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тексты дневников с удаленными стоп-словами, порядок остальных слов сохранен\n",
    "def without_stopwords(name):\n",
    "    with open('Prepared/pr_' + name + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "        diary = f.read().split(' ')\n",
    "    with open('stop_words.txt', 'r', encoding = 'utf-8') as f:\n",
    "        stop = f.read().split('\\n')\n",
    "    for i in range (0, len(diary)):\n",
    "        if len(diary[i]) < 3 or diary[i] in stop:\n",
    "            diary[i] = ''\n",
    "    with open('Prepared/pr_' + name + '.txt', 'w', encoding = 'utf-8') as f:\n",
    "        f.write(' '.join(diary))    \n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#предобработка\n",
    "for i in range(1900, 2000):\n",
    "    filename = 'diaries' + str(i)\n",
    "    diary_parsing(filename)\n",
    "    diary_lemmatize(filename)\n",
    "    word_counter(filename)\n",
    "    without_stopwords(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#частые слова, tf-idf\n",
    "def commonword(diaries):\n",
    "    tf_idf = TfidfVectorizer()\n",
    "    tf_idf.fit(diaries)\n",
    "    idfs = tf_idf.idf_\n",
    "    lower_thresh = 3\n",
    "    often = idfs < lower_thresh\n",
    "    commonwords = np.array(tf_idf.get_feature_names())[often]\n",
    "    return commonwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#очистка от частых слов\n",
    "def clear_diaries(diaries):\n",
    "    common = commonword(diaries)\n",
    "    for i in range(0, len(diaries)):\n",
    "        d = diaries[i].split(\" \")\n",
    "        for j in range(0, len(d)):\n",
    "            if d[j] in common:\n",
    "                d[j] = ''\n",
    "        diaries[i] = \" \".join(d)\n",
    "    return diaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#открываем дневники по десятилетиям\n",
    "def get_diaries(year1, year2):\n",
    "    diaries = []\n",
    "    for i in range(year1, year2):\n",
    "        filename = 'diaries' + str(i)\n",
    "        with open('Prepared/pr_' + filename + '.txt', 'r', encoding = 'utf-8') as f:\n",
    "            d = f.read().split('   лет     ')\n",
    "            for d1 in d:\n",
    "                diaries.append(d1)\n",
    "    for j in range (0, len(diaries)):\n",
    "        diary = diaries[j].split(' ')[5:]\n",
    "        '''morph = pymorphy2.MorphAnalyzer()\n",
    "        for i in range(0, len(diary)):\n",
    "            if morph.parse(diary[i])[0].tag.POS == \"NOUN\":\n",
    "                diary[i] = morph.parse(diary[i].lower())[0].normal_form\n",
    "            else:\n",
    "                diary[i] = ''\n",
    "        '''\n",
    "        diaries[j] = ' '.join(diary)\n",
    "    return clear_diaries(diaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_array = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        outp = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        topic_array.append(outp.split(' '))\n",
    "    return topic_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -152500.49285957788\n",
      "Model Perplexity:  774.0404323008058\n"
     ]
    }
   ],
   "source": [
    "#подбор оптимальных параметров\n",
    "diaries = get_diaries(1900, 1910)\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "tf = tf_vectorizer.fit_transform(diaries)\n",
    "\n",
    "search_params = {'n_components': [5, 10, 15, 20], 'learning_decay': [.5, .7, .9]}\n",
    "lda = LatentDirichletAllocation()\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "model.fit(tf)\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'n_topics': [5, 10, 15, 20], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)\n",
    "\n",
    "best_lda_model = model.best_estimator_\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\polin\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "nmflist = []\n",
    "ldalist = []\n",
    "no_topics = 5\n",
    "for i in range(0, 10):\n",
    "    year1 = 1900 + 10*i\n",
    "    diaries = get_diaries(year1, year1 + 10)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(diaries)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "    tf = tf_vectorizer.fit_transform(diaries)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "    nmflist.append(display_topics(nmf, tfidf_feature_names, 10))\n",
    "    ldalist.append(display_topics(lda, tf_feature_names, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacc(lda_topic, nmf_topic):\n",
    "    arr = []\n",
    "    for i in range (0, len(lda_topic)):\n",
    "        arr1 = []\n",
    "        for j in range (0, len(nmf_topic)):\n",
    "            c = len(set(lda_topic[i]) & set(nmf_topic[j]))\n",
    "            k = round(c / (len(lda_topic[i]) + len(nmf_topic[j]) - c), 2)\n",
    "            arr1.append(k)\n",
    "        arr.append(arr1)\n",
    "    return arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
